# Sampling from multimodal distributions

Let say you have a multimodal distribution, for example a mixture of two Gaussians.
`DifferentialEvolutionMetropolis` implements differential evolution MCMC samplers (including deMC-zs and DREAMz) that are designed to sample from such distributions efficiently.
Roughly these samplers work by generating new proposals based on many separate chains (or a history of sampled chains).
In theory this allows the sampler to easily jump between modes of the distribution.

## Multimodal Distributions

First we need to implement a multimodal problem. We will assume that our data is generated by a mixture of two 2D-Gaussians with means 5 and -5 and a 65% bias towards the positive value. Then the distributions of the means will be multimodal and highly correlated with the probability. We can easily implement this using the `Distributions` package, which underpins most of the functionality in `DifferentialEvolutionMetropolis`.

```@example MMSampler
using Distributions, Random

mean = 5.0
std = 2.0
p = 0.65

dist = MixtureModel([
    Normal(mean, std),
    Normal(-mean,  std)
], [p, 1 - p]);

#generate our data
Random.seed!(1234)
data = rand(dist, 1000)

# setup our log density
struct MixtureNormal
    #data
    data::Vector{Float64}
    #prior parameters
    prior_Œº_œÉ::Float64
    prior_œÉ_œÉ::Float64
end

#initialize the model
model = MixtureNormal(data, 10.0, 10.0)

# implement the log density function
function (problem::MixtureNormal)(Œ∏)
    (; Œº, p, œÉ) = Œ∏
    ld = 
        #priors
        logpdf(Normal(0, problem.prior_œÉ_œÉ), œÉ) + #half-normal because œÉ > 0
        logpdf(Uniform(0, 1), p) #prior on mixture weight
    Œº_prior = Normal(0, problem.prior_Œº_œÉ)
    for Œº_ in Œº
        ld += logpdf(Œº_prior, Œº_)
    end
    #likelihood
    ll_model = MixtureModel(
        [
            Normal(Œº[1], œÉ),
            Normal(Œº[2], œÉ)
        ], [p, 1 - p]
    )
    for i in axes(problem.data, 1)
        ld += logpdf(ll_model, data[i])
    end
    return ld
end

# to visualize our data
using Plots
plot(histogram(data, bins = 30, xlabel = "Value", ylabel = "Frequency", title = "Histogram of Generated Data"))
```

We can also transform our log density function, so we can provide real-valued inputs. This is much easier to work with.

```@example MMSampler
using TransformedLogDensities, TransformVariables

transformation = as((Œº = as(Array, 2), p = asùïÄ, œÉ = as‚Ñù‚Çä))
transformed_ld = TransformedLogDensity(transformation, model)
```

## Sampling with DifferentialEvolutionMetropolis

Now let's use `DifferentialEvolutionMetropolis` to sample from this multimodal distribution. Here we use the DREAMz sampler, which is well-suited for exploring complex, multimodal spaces. We increase the number of chains to allow the sampler to explore the distribution more effectively.

```@example MMSampler
using DifferentialEvolutionMetropolis, AbstractMCMC

model = AbstractMCMC.LogDensityModel(transformed_ld)

# Sample using DREAMz with adaptive stopping based on convergence
dreamz = DREAMz(model, 10000; n_chains = 6, progress = true);
```

Other implementations of the differential evolution MCMC algorithm are available in `DifferentialEvolutionMetropolis.jl`, such as `deMC` and `deMCzs`, which can be used similarly.

## Custom Scheme

DREAMz can be further customized. For example, we could include snooker updates alongside the DREAMz-like subspace sampling.

You can also modify aspects of the implemented sampling, for example tell DREAMz to use non-memory-based sampling with `DREAMz(...; memory = false)`, or you can define your own sampler scheme for more control over the sampling process.

This time we will use `MCMCChains.jl` to the handle the output.

```@example MMSampler
using MCMCChains

# Create a custom sampler scheme combining different update types
custom_sampler = setup_sampler_scheme(
    setup_subspace_sampling(), # a DREAM-like sampler that uses subspace sampling
    setup_snooker_update(deterministic_Œ≥ = false), # a snooker update for better exploration
    setup_de_update(); # standard DE update
    w = [0.6, 0.2, 0.2] # weights for each update type
);

# Sample using AbstractMCMC.sample with custom stopping criteria
custom_results = sample(
    model,
    custom_sampler,
    rÃÇ_stopping_criteria;
    check_every = 10000,
    maximum_RÃÇ = 1.05,
    n_chains = 4,
    N‚ÇÄ = 100, # want good exploration of the space
    memory = true,
    parallel = true,
    annealing = true,
    num_warmup = 10000,
    memory_size = 5000,
    memory_refill = true,
    chain_type = MCMCChains.Chains
);
```

You can also define your own samplers for more specialized use cases by extending the abstract types.

## Interpreting Results

After running the sampler, you will have a collection of samples from the target distribution. These samples can be used to estimate summary statistics, credible intervals, and to assess the quality of your sampling.

### Assessing Sampler Performance: ESS and R-hat

To evaluate how well your sampler is performing, you can compute the effective sample size (ESS) and the R-hat diagnostic. These metrics help you determine if your chains have mixed well and if your estimates are reliable.

- **Effective Sample Size (ESS):** This measures the number of independent samples your chains are equivalent to. Higher ESS values indicate more reliable estimates.
- **R-hat Diagnostic:** Also known as the Gelman-Rubin statistic, R-hat compares the variance within each chain to the variance between chains. Values close to 1 suggest good mixing and convergence; values much greater than 1 indicate potential problems.

Below is an example of how to compute these diagnostics for the DifferentialEvolutionMetropolis samplers using `MCMCDiagnosticTools`:

```@example MMSampler
using Statistics, MCMCDiagnosticTools

# Compute diagnostics for DREAMz results
ess_val = ess(dreamz.samples) ./ size(dreamz.samples, 1)
rhat_val = maximum(rhat(dreamz.samples))

println("DREAMz diagnostics:")
println("  ESS per iteration: $ess_val")
println("  R-hat: $rhat_val")

# example trace plot
plot(dreamz.samples[:, :, 1], xlabel = "Iteration", ylabel = "Value", title = "Trace Plot for Mean 1 (DREAMz)")
```

### Summarizing Posterior Samples

Once you have confirmed good mixing and convergence, you can summarize your posterior samples. For each parameter, you may want to compute the median and a credible interval (such as the 90% interval):

```@example MMSampler
#need to transform the samples to original space, there is currently no nice way to do this
custom_results = cat([dreamz.samples[:, c, :] for c in axes(dreamz.samples, 2)]...; dims = 1)
for it in axes(custom_results, 1)
    transformed_values = transform(transformation, Array(custom_results[it, :]))
    custom_results[it, :] .= [transformed_values.Œº..., transformed_values.p, transformed_values.œÉ]
end

#median for variance
med = median(custom_results[:, 4])
q05, q95 = quantile(custom_results[:, 4], [0.05, 0.95])
println("Standard deviation: posterior median = $med, 90% CI = ($q05, $q95)")
println("True standard deviation: $std")

#not so sensible for p or means
med = median(custom_results[:, 1])
q05, q95 = quantile(custom_results[:, 1], [0.05, 0.95])
println("Mean 1: posterior median = $med, 90% CI = ($q05, $q95)")
println("True mean: $(mean)")

#visualize the posteriors, expect multimodal distributions
plot(histogram(custom_results[:, 1], bins = 30, xlabel = "Mean 1", ylabel = "Frequency"), histogram(custom_results[:, 2], bins = 30, xlabel = "Mean 2", ylabel = "Frequency"), histogram(custom_results[:, 3], bins = 30, xlabel = "Probability", ylabel = "Frequency"), layout = (3, 1))

#covariance
plot(scatter(custom_results[:, 1], custom_results[:, 3], xlabel = "Mean 1", ylabel = "Probability of Normal 1"), scatter(custom_results[:, 1], custom_results[:, 2], xlabel = "Mean 1", ylabel = "Mean 2"), layout = (2, 1))
```

For more details, see the [DifferentialEvolutionMetropolis Documentation](@ref) and [Customizing your sampler](@ref).
