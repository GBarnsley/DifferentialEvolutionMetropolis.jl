var documenterSearchIndex = {"docs":
[{"location":"custom/#Customizing-your-sampler","page":"Customizing your sampler","title":"Customizing your sampler","text":"This document describes how to extend DifferentialEvolutionMetropolis.jl with your own custom components. You can define custom stopping criteria, diagnostic checks, and proposal distributions (updates).","category":"section"},{"location":"custom/#Custom-Stopping-Criteria","page":"Customizing your sampler","title":"Custom Stopping Criteria","text":"To create a custom stopping criterion, you need to define a function that follows the AbstractMCMC interface, similar to r̂_stopping_criteria which is available if MCMCDiagnosticTools.jl is loaded. The function will be called during sampling to determine when to stop. See the AbstractMCMC documentation for details.\n\nThe stopping criterion function has the following signature:\n\nfunction your_stopping_criteria(\n    rng::AbstractRNG,\n    model::AbstractModel, \n    sampler::AbstractDifferentialEvolutionSampler,\n    samples::Vector{DifferentialEvolutionSample},\n    state::DifferentialEvolutionState,\n    iteration::Int;\n    kwargs...\n)\n\nrng: Random number generator (required by AbstractMCMC interface, usually unused)\nmodel: The model being sampled (required by interface, usually unused)  \nsampler: The differential evolution sampler (required by interface, usually unused)\nsamples: Vector of all collected samples from all chains\nstate: Current sampler state (required by interface, usually unused)\niteration: Current iteration number\nkwargs...: Additional keyword arguments passed via AbstractMCMC.sample\n\nThe function should return true if sampling should stop, and false otherwise.\n\nHere is an example of a very simple stopping criterion that stops sampling after a maximum number of iterations has been reached:\n\nusing DifferentialEvolutionMetropolis, AbstractMCMC\n\nfunction max_iterations_stopping(\n    rng::AbstractRNG,\n    model::AbstractModel,\n    sampler::AbstractDifferentialEvolutionSampler, \n    samples::Vector{DifferentialEvolutionSample{V, VV}},\n    state::DifferentialEvolutionState{T, V, VV, A},\n    iteration::Int;\n    max_iterations::Int = 10000,\n    kwargs...\n) where {T<:Real, V<:AbstractVector{T}, VV<:AbstractVector{V}, A<:AbstractDifferentialEvolutionAdaptiveState{T}}\n    if iteration >= max_iterations\n        println(\"Reached maximum iterations ($max_iterations), stopping.\")\n        return true\n    end\n    return false\nend\n\n# Usage with AbstractMCMC.sample\nusing LogDensityProblems\n\nmodel = LogDensityModel(your_log_density)\nsampler = setup_de_update()\n\nresult = sample(\n    rng, \n    model, \n    sampler, \n    max_iterations_stopping;\n    max_iterations = 5000,  # passed as keyword argument\n    n_chains = 4\n)","category":"section"},{"location":"custom/#Custom-Proposal-Distributions","page":"Customizing your sampler","title":"Custom Proposal Distributions","text":"You can create your own proposal distributions by defining a new sampler type that subtypes AbstractDifferentialEvolutionSampler and implementing the proposal method.\n\nThe method signature for the proposal is:\n\nfunction proposal!(\n    state::DifferentialEvolutionMetropolis.DifferentialEvolutionState, \n    sampler::YourSampler, \n    current_state::Int\n)\n\nstate: The current state containing all chain positions, log-densities, and chain specific rngs\nsampler: An instance of your custom sampler struct\ncurrent_state: The index of the chain to be updated\n\nThe function should modify state.xₚ[current_state] = proposed_position and return a named tuple with at least (offset = hastings_correction) where:\n\nproposed_position: The proposed new position (vector)\noffset: Hastings ratio correction in log-space (typically 0.0 for symmetric proposals)\n\nHere is an example of a simple Metropolis-Hastings random walk update with a fixed step size:\n\nusing DifferentialEvolutionMetropolis, Distributions, Random\n\n# Define the struct for the sampler\nstruct MetropolisHastingsUpdate <: DifferentialEvolutionMetropolis.AbstractDifferentialEvolutionSampler\n    proposal_distribution::MvNormal\nend\n\n# Implement the proposal function\nfunction DifferentialEvolutionMetropolis.proposal!(\n    state::DifferentialEvolutionMetropolis.DifferentialEvolutionState,\n    sampler::MetropolisHastingsUpdate,\n    current_state::Int\n)\n    # Get the current position of this chain\n    x_current = state.x[current_state]\n    \n    # Propose a new point (stored in state) using a random walk\n    state.xₚ[current_state] .= x_current .+ rand(state.rngs[current_state], sampler.proposal_distribution)\n    \n    # The proposal is symmetric, so no Hastings correction needed\n    return (offset = 0.0)\nend","category":"section"},{"location":"custom/#Adaptive-Proposals-with-step_warmup","page":"Customizing your sampler","title":"Adaptive Proposals with step_warmup","text":"For proposals that require adaptation during warm-up, you need to implement the step_warmup as well. This is called during the warm-up phase. Unless you want your sampler to be always adaptive then you must implement step.\n\nYou'll also need to define adaptive state structures and methods. Here's an example of an adaptive Metropolis-Hastings sampler:\n\nusing AbstractMCMC, DifferentialEvolutionMetropolis\n# Define adaptive state\nstruct AdaptiveMetropolisState{T<:Real} <:DifferentialEvolutionMetropolis.AbstractDifferentialEvolutionAdaptiveState{T}\n    proposal_cov::Matrix{T}\n    adaptation_count::Int\n    running_mean::Vector{T}\n    running_cov::Matrix{T}\nend\n\n# Define the adaptive sampler  \nstruct AdaptiveMetropolisUpdate{T<:Real} <: DifferentialEvolutionMetropolis.AbstractDifferentialEvolutionSampler\n    initial_cov::Matrix{T}\n    adapt_after::Int\n    adapt_every::Int\n    adapt_scale::T\nend\n\n# Constructor\nfunction AdaptiveMetropolisUpdate(\n    n_params::Int;\n    initial_std::Float64 = 0.1,\n    adapt_after::Int = 200,\n    adapt_every::Int = 100,\n    adapt_scale::Float64 = 2.38^2\n)\n    initial_cov = (initial_std^2) * I(n_params)\n    return AdaptiveMetropolisUpdate{Float64}(initial_cov, adapt_after, adapt_every, adapt_scale / n_params)\nend\n\n# Initialize adaptive state\nfunction DifferentialEvolutionMetropolis.initialize_adaptive_state(sampler::AdaptiveMetropolisUpdate{T}, model_wrapper::AbstractMCMC.LogDensityModel, n_chains::Int) where {T}\n    n_params = dimension(model_wrapper.logdensity)\n    return AdaptiveMetropolisState{T}(\n        copy(sampler.initial_cov),\n        0,\n        zeros(T, n_params),\n        copy(sampler.initial_cov)\n    )\nend\n\n# Fix sampler (convert adaptive to non-adaptive)\nfunction DifferentialEvolutionMetropolis.fix_sampler(sampler::AdaptiveMetropolisUpdate{T}, adaptive_state::AdaptiveMetropolisState{T}) where {T}\n    return MetropolisHastingsUpdate(MvNormal(zeros(T, size(adaptive_state.proposal_cov, 1)), adaptive_state.proposal_cov))\nend\n\n# Proposal method (same as non-adaptive)\nfunction DifferentialEvolutionMetropolis.proposal!(\n    state::DifferentialEvolutionMetropolis.DifferentialEvolutionState,\n    sampler::AdaptiveMetropolisUpdate,\n    current_state::Int\n)\n    x_current = state.x[current_state]\n    # Use current proposal covariance from adaptive state\n    state.xₚ[current_state] .= rand(rng, MvNormal(x_current, state.adaptive_state.proposal_cov))\n    return (offset = 0.0)\nend\n\n# Adaptive step during warm-up\nfunction step_warmup(\n    rng::AbstractRNG,\n    model_wrapper::AbstractMCMC.LogDensityModel,\n    sampler::AdaptiveMetropolisUpdate{T},\n    state::DifferentialEvolutionMetropolis.DifferentialEvolutionState{T, AdaptiveMetropolisState{T}};\n    parallel::Bool = false,\n    kwargs...\n) where {T<:Real}\n    \n    # Perform regular step\n    sample, new_state = step(rng, model_wrapper, sampler, state; parallel = parallel, kwargs...)\n    \n    # Update adaptive parameters\n    adapt_state = new_state.adaptive_state\n    new_count = adapt_state.adaptation_count + 1\n    \n    # Only adapt after burn-in period and at specified intervals\n    if new_count > sampler.adapt_after && new_count % sampler.adapt_every == 0\n        \n        # Compute empirical covariance from current chain positions\n        positions = reduce(hcat, new_state.x)'  # Convert to matrix\n        empirical_cov = cov(positions)\n        \n        # Update proposal covariance with regularization\n        new_proposal_cov = sampler.adapt_scale * empirical_cov + 1e-6 * I\n        \n        # Update adaptive state\n        new_adaptive_state = AdaptiveMetropolisState{T}(\n            new_proposal_cov,\n            new_count,\n            adapt_state.running_mean,  # Could update these too\n            adapt_state.running_cov\n        )\n        \n        return sample, update_state(new_state; adaptive_state = new_adaptive_state)\n    else\n        # Just update the count\n        new_adaptive_state = AdaptiveMetropolisState{T}(\n            adapt_state.proposal_cov,\n            new_count,\n            adapt_state.running_mean,\n            adapt_state.running_cov\n        )\n        return sample, update_state(new_state; adaptive_state = new_adaptive_state)\n    end\nend","category":"section"},{"location":"custom/#Example:-Using-Custom-Components","page":"Customizing your sampler","title":"Example: Using Custom Components","text":"Here is a complete example that shows how to use custom components with the new AbstractMCMC interface:\n\nusing Distributions, TransformedLogDensities, LinearAlgebra, TransformVariables, Plots\n\n# Set up a simple log-density to sample from (a 2D standard normal distribution)\n\nld = TransformedLogDensity(as(Array, 2), x -> -sum(x.^2) / 2)\ndimension(ld) = 2\nmodel = AbstractMCMC.LogDensityModel(ld)\n\n# Create custom samplers\nsimple_mh = MetropolisHastingsUpdate(MvNormal([0.0, 0.0], 0.1 * I))\nadaptive_mh = AdaptiveMetropolisUpdate(2; initial_std = 0.1)\n\n# Create a composite sampler scheme\nmy_sampler_scheme = setup_sampler_scheme(\n    simple_mh, \n    adaptive_mh;\n    w = [0.3, 0.7]  # Use adaptive sampler 70% of the time\n)\n\n# Sample using AbstractMCMC.sample \nresult = sample(\n    Random.default_rng(),\n    model,\n    my_sampler_scheme,\n    5000;\n    n_chains = 6,\n    num_warmup = 10000, #adaptive steps\n    memory = true,\n    parallel = false,\n    chain_type = DifferentialEvolutionOutput\n)\n\nplot(result.samples[:, :, 1])\nplot(result.samples[:, :, 2])","category":"section"},{"location":"#DifferentialEvolutionMetropolis-Documentation","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis Documentation","text":"Tools for sampling from log-densities using differential evolution algorithms.\n\nSee Sampling from multimodal distributions and Customizing your sampler to get started.\n\nThis package is built upon AbstractMCMC.jl so log-densities should be constructed using that package, and can be used with TransformVariables.jl or Bijectors.jl to control the parameter space.\n\nThe other key dependency is Distributions.jl. Almost every parameter in proposals given here are defined via customizable univariate distributions. Values that are fixed are specified via a Dirac distribution, though in the API these can be specified with any real value. As a warning there are some checks on the given distributions, but in the interest of flexibility it is up to the user to ensure that they are suitable for the given parameter. You can disable any checking of your provided distributions with ; check_args = false if you really want to ruin your sampler efficiency. Distributions can optionally be used to define your log-density, as in the examples given here. \n\nAs far as I am aware, there is one other package that implements differential evolution MCMC in Julia, DifferentialEvolutionMCMC.jl. I opted to implement my own version as I wanted a more flexible API and the subsampling scheme from DREAM. That's not to discredit DifferentialEvolutionMCMC.jl, it has many features this package does not, such as being able to work on optimization problems and parameter blocking.","category":"section"},{"location":"#Main-features","page":"DifferentialEvolutionMetropolis Documentation","title":"Main features","text":"Original differential evolution, snooker, and adaptive subspace sampling (i.e. from DREAM) updates\nOptional parallel tempering (no swaps yet, information is shared by the DE updates!) and annealing\nComposite samplers, can combine any of the implemented updates (in future I'll wrap other abstractMCMC based samplers)\nEasy to implement your own updates!\nCan output in MCMCChains format, though you use multiple sampling chains (i.e. chains of the DE-chains) these will all be appended together","category":"section"},{"location":"#Next-Steps","page":"DifferentialEvolutionMetropolis Documentation","title":"Next Steps","text":"A few plans for this package, feel free to suggest features or improvements via issues:\n\nImplement multi-try and delayed rejection DREAM, I avoided these so far since I have been using these samplers for costly log-densities with relatively few parameters, such as one that solve an ODE.\nAdditional diagnostic checks and adaptive schemes.","category":"section"},{"location":"#Contents","page":"DifferentialEvolutionMetropolis Documentation","title":"Contents","text":"","category":"section"},{"location":"#Functions","page":"DifferentialEvolutionMetropolis Documentation","title":"Functions","text":"","category":"section"},{"location":"#Implemented-Sampling-Schemes","page":"DifferentialEvolutionMetropolis Documentation","title":"Implemented Sampling Schemes","text":"","category":"section"},{"location":"#Setup-Functions","page":"DifferentialEvolutionMetropolis Documentation","title":"Setup Functions","text":"","category":"section"},{"location":"#Core-Sampling-Functions","page":"DifferentialEvolutionMetropolis Documentation","title":"Core Sampling Functions","text":"","category":"section"},{"location":"#Convergence-and-Stopping-Criteria","page":"DifferentialEvolutionMetropolis Documentation","title":"Convergence and Stopping Criteria","text":"","category":"section"},{"location":"#Output","page":"DifferentialEvolutionMetropolis Documentation","title":"Output","text":"The output format can be modified with chain_type, the supported options are Chains from MCMCChains, VNChain from FlexiChains, Any which returns the basic DifferentialEvolutionMetropolis.DifferentialEvolutionSample, and the default option DifferentialEvolutionOutput. If save_final_state = true the format will be (sample::requested format, final_state). If run in parallel using step(model, sampler, parallel_option, n_its, n_meta_chains; n_chains = n_chains) the meta chains and DE chains will be merged into one dimension for both Chains and DifferentialEvolutionOutput, if the final state is saved it will be a vector of length n_meta_chains containing the final state for each.\n\nNote that support for FlexiChains is a bit underutilized as the all samplers currently require all of your parameters to have one type.","category":"section"},{"location":"#Index","page":"DifferentialEvolutionMetropolis Documentation","title":"Index","text":"","category":"section"},{"location":"#DifferentialEvolutionMetropolis.deMC","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.deMC","text":"deMC(model_wrapper, n_its; kwargs...)\n\nRun the Differential Evolution Markov Chain (DE-MC) sampler proposed by ter Braak (2006).\n\nThis sampler uses differential evolution updates with optional switching between two scaling factors (γ₁ and γ₂) to enable mode switching.\n\nThis implementation varies slightly from the original: updates within a population occur based on the previous positions to enable easy parallelization.\n\nSee doi.org/10.1007/s11222-006-8769-1 for more information.\n\nThe algorithm runs for a fixed number of iterations with optional burn-in.\n\nArguments\n\nmodel_wrapper: LogDensityModel containing the target log-density function\nn_its: Number of sampling iterations per chain\n\nKeyword Arguments\n\nrng: Random number generator. Defaults to default_rng().\nn_burnin: Number of burn-in iterations. Defaults to n_its * 5.\nsave_burnt: Save burn-in samples in output. Defaults to false.\nγ₁: Primary scaling factor. Defaults to 2.38 / sqrt(2 * dim).\nγ₂: Secondary scaling factor for mode switching. Defaults to 1.0.\np_γ₂: Probability of using γ₂. Defaults to 0.1.\nchain_type: Type of chain to return (e.g., Any, DifferentialEvolutionOutput, MCMCChains.Chains, or FlexiChains.VNChain). Defaults to DifferentialEvolutionOutput.\nsave_final_state: Whether to return the final state along with samples, if true the output will be (samples::chaintype, finalstate). Defaults to false.\n\nGeneric DE Arguments\n\nn_chains: Number of parallel chains. Defaults to max(2 * dimension, 3) for adequate mixing.\nadapt: Whether to enable adaptive behavior during warm-up (if the sampler supports it). Defaults to true.\ninitial_position: Starting positions for chains. Can be nothing (random initialization), or a vector of parameter vectors. If the provided vector is smaller than n_chains + n_hot_chains, it will be expanded; if larger and memory=true, excess positions become initial memory. Defaults to nothing.\nparallel: Whether to evaluate initial log-densities in parallel. Useful for expensive models. Defaults to false.\nn_preallocated_indices: This package provides fast sampling-without-replacement by pre-allocating indices, defaults to 3 (which the most asked for by the implemented samplers). Consider increasing it if you implement your own proposal that calls pick_chains with n_chains > 3.\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Overwrites memory options given at initialization, generally should only be of use if calling step directly.\nsilent: Suppress informational logging during initialization (e.g., initial position adjustments and   memory setup) when true. Defaults to false.\n\nMemory-based Sampling Arguments\n\nmemory: Whether to use memory-based sampling that stores past positions. Memory-based samplers can be more efficient for high-dimensional problems. Defaults to false.\nN₀: Initial memory size for memory-based samplers. Should be ≥ n_chains + n_hot_chains. Defaults to 2 * n_chains + n_hot_chains.\nmemory_size: Maximum number of positions retained per chain in memory. The effective total stored positions is   memory_size * (n_chains + n_hot_chains). Defaults to 1001 or 2*num_warmup if that is provided here or via sample. Larger sizes can improve proposal diversity but   increase memory usage. Set with consideration of available RAM and expected run length.\nmemory_refill: Whether to refill memory when full instead of extending the memory, will replace from the start. Defaults to false.\nmemory_thin_interval: Thinning interval for memory updates. If > 0, only every memory_thin_interval-th position is stored in memory.\n\nParallel Tempering and Simulated Annealing Arguments\n\nn_hot_chains: Number of hot chains for parallel tempering. Defaults to 0 (no parallel tempering).\nmax_temp_pt: Maximum temperature for parallel tempering. Defaults to 2*sqrt(dimension).\nmax_temp_sa: Maximum temperature for simulated annealing. Defaults to max_temp_pt.\nα: Temperature ladder spacing parameter. Controls the geometric spacing between temperatures. Defaults to 1.0.\nannealing: Whether to use simulated annealing (temperature decreases over time). Defaults to false.\nannealing_steps: Number of annealing steps. Defaults to annealing ? num_warmup : 0.\ntemperature_ladder: Pre-defined temperature ladder as a vector of vectors. If provided, overrides automatic temperature ladder creation. Defaults to create_temperature_ladder(n_chains, n_hot_chains, α, max_temp_pt, max_temp_sa, annealing_steps).\n\nkwargs...: Additional keyword arguments passed to AbstractMCMC.sample (e.g., memory_refill, memory_thin_interval, silent). See AbstractMCMC documentation.\n\nReturns\n\ndepends on chain_type, and save_final_state\n\nExample\n\nusing DifferentialEvolutionMetropolis, Random, Distributions\n\n# Define a simple log-density function\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\n\n# Run differential evolution MCMC\nresult = deMC(model_wrapper, 1000; n_chains = 10, parallel = false)\n\nNotes\n\nFor non-memory samplers, n_chains should typically be ≥ dimension for good mixing\nMemory-based samplers can work effectively with fewer chains than the problem dimension\nThe function handles dimension mismatches and provides informative warnings\nInitial log-densities are computed automatically for all starting positions\nWhen using parallel tempering (n_hot_chains > 0), only the cold chains (first n_chains) are returned in the sample, but all chains participate in the sampling process\nMemory-based samplers with parallel tempering may issue warnings since hot chains typically aren't necessary when using memory\n\nSee also deMCzs, DREAMz, setup_de_update.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.deMCzs","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.deMCzs","text":"deMCzs(model_wrapper, n_its; kwargs...)\n\nRun the Differential Evolution Markov Chain with snooker update and historic sampling (DE-MCzs) sampler.\n\nIt combines DE updates with optional snooker moves and uses memory-based sampling to efficiently handle high-dimensional problems with fewer chains.\n\nProposed by ter Braak and Vrugt (2008), see doi.org/10.1007/s11222-008-9104-9.\n\nThe algorithm runs for a fixed number of iterations with optional burn-in.\n\nArguments\n\nmodel_wrapper: LogDensityModel containing the target log-density function\nn_its: Number of sampling iterations per chain\n\nKeyword Arguments\n\nrng: Random number generator. Defaults to default_rng().\nn_burnin: Number of burn-in iterations. Defaults to n_its * 5.\nγ: Scaling factor for DE updates. Defaults to 2.38 / sqrt(2 * dim).\nγₛ: Scaling factor for snooker updates. Defaults to 2.38 / sqrt(2).\np_snooker: Probability of snooker moves. Defaults to 0.1.\nβ: Noise distribution for DE updates. Defaults to Uniform(-1e-4, 1e-4).\nchain_type: Type of chain to return (e.g., Any, DifferentialEvolutionOutput, MCMCChains.Chains, or FlexiChains.VNChain). Defaults to DifferentialEvolutionOutput.\nsave_final_state: Whether to return the final state along with samples, if true the output will be (samples::chaintype, finalstate). Defaults to false.\n\nGeneric DE Arguments\n\nn_chains: Number of parallel chains. Defaults to max(2 * dimension, 3) for adequate mixing.\nadapt: Whether to enable adaptive behavior during warm-up (if the sampler supports it). Defaults to true.\ninitial_position: Starting positions for chains. Can be nothing (random initialization), or a vector of parameter vectors. If the provided vector is smaller than n_chains + n_hot_chains, it will be expanded; if larger and memory=true, excess positions become initial memory. Defaults to nothing.\nparallel: Whether to evaluate initial log-densities in parallel. Useful for expensive models. Defaults to false.\nn_preallocated_indices: This package provides fast sampling-without-replacement by pre-allocating indices, defaults to 3 (which the most asked for by the implemented samplers). Consider increasing it if you implement your own proposal that calls pick_chains with n_chains > 3.\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Overwrites memory options given at initialization, generally should only be of use if calling step directly.\nsilent: Suppress informational logging during initialization (e.g., initial position adjustments and   memory setup) when true. Defaults to false.\n\nMemory-based Sampling Arguments\n\nmemory: Whether to use memory-based sampling that stores past positions. Memory-based samplers can be more efficient for high-dimensional problems. Defaults to true.\nN₀: Initial memory size for memory-based samplers. Should be ≥ n_chains + n_hot_chains. Defaults to 2 * n_chains + n_hot_chains.\nmemory_size: Maximum number of positions retained per chain in memory. The effective total stored positions is   memory_size * (n_chains + n_hot_chains). Defaults to 1001 or 2*num_warmup if that is provided here or via sample. Larger sizes can improve proposal diversity but   increase memory usage. Set with consideration of available RAM and expected run length.\nmemory_refill: Whether to refill memory when full instead of extending the memory, will replace from the start. Defaults to false.\nmemory_thin_interval: Thinning interval for memory updates. If > 0, only every memory_thin_interval-th position is stored in memory.\n\nParallel Tempering and Simulated Annealing Arguments\n\nn_hot_chains: Number of hot chains for parallel tempering. Defaults to 0 (no parallel tempering).\nmax_temp_pt: Maximum temperature for parallel tempering. Defaults to 2*sqrt(dimension).\nmax_temp_sa: Maximum temperature for simulated annealing. Defaults to max_temp_pt.\nα: Temperature ladder spacing parameter. Controls the geometric spacing between temperatures. Defaults to 1.0.\nannealing: Whether to use simulated annealing (temperature decreases over time). Defaults to false.\nannealing_steps: Number of annealing steps. Defaults to annealing ? num_warmup : 0.\ntemperature_ladder: Pre-defined temperature ladder as a vector of vectors. If provided, overrides automatic temperature ladder creation. Defaults to create_temperature_ladder(n_chains, n_hot_chains, α, max_temp_pt, max_temp_sa, annealing_steps).\n\nkwargs...: Additional keyword arguments passed to AbstractMCMC.sample (e.g., memory_refill, memory_thin_interval, silent). See AbstractMCMC documentation.\n\nReturns\n\ndepends on chain_type, and save_final_state\n\nExample\n\nusing DifferentialEvolutionMetropolis, Random, Distributions\n\n# Define a simple log-density function\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\n\n# Run differential evolution MCMC\nresult = deMCzs(model_wrapper, 1000; n_chains = 3)\n\nNotes\n\nFor non-memory samplers, n_chains should typically be ≥ dimension for good mixing\nMemory-based samplers can work effectively with fewer chains than the problem dimension\nThe function handles dimension mismatches and provides informative warnings\nInitial log-densities are computed automatically for all starting positions\nWhen using parallel tempering (n_hot_chains > 0), only the cold chains (first n_chains) are returned in the sample, but all chains participate in the sampling process\nMemory-based samplers with parallel tempering may issue warnings since hot chains typically aren't necessary when using memory\n\nSee also deMC, DREAMz.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.DREAMz","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.DREAMz","text":"DREAMz(model_wrapper, n_its; kwargs...)\n\nRun the Differential Evolution Adaptive Metropolis (DREAMz) sampler.\n\nThis advanced adaptive sampler uses subspace sampling with adaptive crossover probabilities. It can switch between scaling factors and includes outlier chain detection/replacement. The algorithm adapts during warm-up and can use memory-based sampling for efficiency.\n\nBased on Vrugt et al. (2009), see doi.org/10.1515/IJNSNS.2009.10.3.273.\n\nThe algorithm runs for a fixed number of iterations with optional burn-in.\n\nArguments\n\nmodel_wrapper: LogDensityModel containing the target log-density function\nn_its: Number of sampling iterations per chain\n\nKeyword Arguments\n\nrng: Random number generator. Defaults to default_rng().\nn_burnin: Number of burn-in iterations. Defaults to n_its * 5.\nγ₁: Primary scaling factor for subspace updates. Defaults to adaptive.\nγ₂: Secondary scaling factor. Defaults to 1.0.\np_γ₂: Probability of using γ₂. Defaults to 0.2.\nn_cr: Number of crossover probabilities for adaptation. Defaults to 3.\ncr₁: Crossover probability for γ₁. Defaults to adaptive.\ncr₂: Crossover probability for γ₂. Defaults to adaptive.\nϵ: Additive noise distribution. Defaults to Uniform(-1e-4, 1e-4).\ne: Multiplicative noise distribution. Defaults to Normal(0.0, 1e-2).\nchain_type: Type of chain to return (e.g., Any, DifferentialEvolutionOutput, MCMCChains.Chains, or FlexiChains.VNChain). Defaults to DifferentialEvolutionOutput.\nsave_final_state: Whether to return the final state along with samples, if true the output will be (samples::chaintype, finalstate). Defaults to false.\n\nGeneric DE Arguments\n\nn_chains: Number of parallel chains. Defaults to max(2 * dimension, 3) for adequate mixing.\nadapt: Whether to enable adaptive behavior during warm-up (if the sampler supports it). Defaults to true.\ninitial_position: Starting positions for chains. Can be nothing (random initialization), or a vector of parameter vectors. If the provided vector is smaller than n_chains + n_hot_chains, it will be expanded; if larger and memory=true, excess positions become initial memory. Defaults to nothing.\nparallel: Whether to evaluate initial log-densities in parallel. Useful for expensive models. Defaults to false.\nn_preallocated_indices: This package provides fast sampling-without-replacement by pre-allocating indices, defaults to 3 (which the most asked for by the implemented samplers). Consider increasing it if you implement your own proposal that calls pick_chains with n_chains > 3.\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Overwrites memory options given at initialization, generally should only be of use if calling step directly.\nsilent: Suppress informational logging during initialization (e.g., initial position adjustments and   memory setup) when true. Defaults to false.\n\nMemory-based Sampling Arguments\n\nmemory: Whether to use memory-based sampling that stores past positions. Memory-based samplers can be more efficient for high-dimensional problems. Defaults to true.\nN₀: Initial memory size for memory-based samplers. Should be ≥ n_chains + n_hot_chains. Defaults to 2 * n_chains + n_hot_chains.\nmemory_size: Maximum number of positions retained per chain in memory. The effective total stored positions is   memory_size * (n_chains + n_hot_chains). Defaults to 1001 or 2*num_warmup if that is provided here or via sample. Larger sizes can improve proposal diversity but   increase memory usage. Set with consideration of available RAM and expected run length.\nmemory_refill: Whether to refill memory when full instead of extending the memory, will replace from the start. Defaults to false.\nmemory_thin_interval: Thinning interval for memory updates. If > 0, only every memory_thin_interval-th position is stored in memory.\n\nParallel Tempering and Simulated Annealing Arguments\n\nn_hot_chains: Number of hot chains for parallel tempering. Defaults to 0 (no parallel tempering).\nmax_temp_pt: Maximum temperature for parallel tempering. Defaults to 2*sqrt(dimension).\nmax_temp_sa: Maximum temperature for simulated annealing. Defaults to max_temp_pt.\nα: Temperature ladder spacing parameter. Controls the geometric spacing between temperatures. Defaults to 1.0.\nannealing: Whether to use simulated annealing (temperature decreases over time). Defaults to false.\nannealing_steps: Number of annealing steps. Defaults to annealing ? num_warmup : 0.\ntemperature_ladder: Pre-defined temperature ladder as a vector of vectors. If provided, overrides automatic temperature ladder creation. Defaults to create_temperature_ladder(n_chains, n_hot_chains, α, max_temp_pt, max_temp_sa, annealing_steps).\n\nkwargs...: Additional keyword arguments passed to AbstractMCMC.sample (e.g., memory_refill, memory_thin_interval, silent). See AbstractMCMC documentation.\n\nReturns\n\ndepends on chain_type, and save_final_state\n\nExample\n\nusing DifferentialEvolutionMetropolis, Random, Distributions\n\n# Define a simple log-density function\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\n\n# Run DREAM with subspace sampling\nresult = DREAMz(model_wrapper, 1000; n_chains = 10, memory = false)\n\nNotes\n\nFor non-memory samplers, n_chains should typically be ≥ dimension for good mixing\nMemory-based samplers can work effectively with fewer chains than the problem dimension\nThe function handles dimension mismatches and provides informative warnings\nInitial log-densities are computed automatically for all starting positions\nWhen using parallel tempering (n_hot_chains > 0), only the cold chains (first n_chains) are returned in the sample, but all chains participate in the sampling process\nMemory-based samplers with parallel tempering may issue warnings since hot chains typically aren't necessary when using memory\n\nSee also deMC, deMCzs, setup_subspace_sampling.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.setup_sampler_scheme","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.setup_sampler_scheme","text":"Create a composite sampler scheme from multiple differential evolution update steps.\n\nThe update method used in each iteration for each chain is randomly selected from the provided update steps according to their weights. This allows combining different sampling strategies (e.g., DE updates with snooker updates) in a single sampler.\n\nArguments\n\nupdates...: One or more differential evolution sampler objects created by functions like setup_de_update, setup_snooker_update, setup_subspace_sampling.\n\nKeyword Arguments\n\nw: Vector of weights for each update step. If not provided, all updates are chosen with equal probability. Weights must be non-negative and will be automatically normalized.\n\nReturns\n\nA DifferentialEvolutionCompositeSampler that can be used with AbstractMCMC.sample.\n\nExamples\n\nusing DifferentialEvolutionMetropolis\n\n# Only snooker updates\nsampler1 = setup_sampler_scheme(setup_snooker_update())\n\n# DE and Snooker with equal probability\nsampler2 = setup_sampler_scheme(setup_de_update(), setup_snooker_update())\n\n# Snooker 10% of the time, DE 90% of the time\nsampler3 = setup_sampler_scheme(setup_de_update(), setup_snooker_update(); w = [0.9, 0.1])\n\nSee also setup_de_update, setup_snooker_update, setup_subspace_sampling.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.setup_de_update","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.setup_de_update","text":"Set up a Differential Evolution (DE) update step for MCMC sampling.\n\nCreates a sampler that proposes new states by adding scaled difference vectors between randomly selected chains plus small noise. This is the core update mechanism from the original DE-MC algorithm by ter Braak (2006).\n\nSee doi.org/10.1007/s11222-006-8769-1 for more information.\n\nKeyword Arguments\n\nγ: Scaling factor for the difference vector. Can be a Real (fixed value), a UnivariateDistribution (random scaling), or nothing (automatic based on n_dims). Defaults to nothing.\nβ: Distribution for small noise added to proposals. Must be a univariate continuous distribution. Defaults to Uniform(-1e-4, 1e-4).\nn_dims: Problem dimension used for automatic γ selection. If > 0 and γ is nothing, sets γ to the theoretically optimal 2.38 / sqrt(2 * n_dims). If ≤ 0, uses Uniform(0.8, 1.2). Defaults to 0.\ncheck_args: Whether to validate input distributions. Defaults to true.\n\nReturns\n\nA DifferentialEvolutionSampler that can be used with setup_sampler_scheme or step or sample from AbstractMCMC.\n\nExample\n\nusing DifferentialEvolutionMetropolis, Distributions\n\n# Setup differential evolution update with custom parameters\nde_update = setup_de_update(γ = 1.0, β = Normal(0.0, 0.01))\n\nSee also setup_snooker_update, setup_subspace_sampling, setup_sampler_scheme.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.setup_snooker_update","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.setup_snooker_update","text":"Set up a Snooker update step for MCMC sampling.\n\nCreates a sampler that proposes moves along the line connecting the current position to a projection point, scaled by the difference between two other randomly selected chains. This update can help with sampling from distributions with complex geometries by making larger moves in effective directions.\n\nSee doi.org/10.1007/s11222-008-9104-9 for more information.\n\nKeyword Arguments\n\nγ: Scaling factor for the projection. Can be a Real (fixed value), a UnivariateDistribution (random scaling), or nothing (automatic based on deterministic_γ). Defaults to nothing.\ndeterministic_γ: When γ is nothing, determines the automatic value. If true, uses the theoretically optimal 2.38 / sqrt(2). If false, uses Uniform(0.8, 1.2). Defaults to true.\ncheck_args: Whether to validate input distributions. Defaults to true.\n\nReturns\n\nA DifferentialEvolutionSnookerSampler that can be used with setup_sampler_scheme or step or sample from AbstractMCMC.\n\nExample\n\nusing DifferentialEvolutionMetropolis, Distributions\n\n# Setup snooker update with custom gamma distribution\nsnooker_update = setup_snooker_update(γ = Uniform(0.1, 2.0))\n\nSee also setup_de_update, setup_subspace_sampling, setup_sampler_scheme.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.setup_subspace_sampling","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.setup_subspace_sampling","text":"Set up a Subspace Sampling (DREAM-like) update step for MCMC sampling.\n\nCreates a sampler that updates only a random subset of parameters in each iteration, using multiple scaled difference vectors. The crossover probability determines which parameters to update and can be adapted during warm-up for improved efficiency.\n\nSee doi.org/10.1515/IJNSNS.2009.10.3.273 for more information.\n\nKeyword Arguments\n\nγ: Scaling factor for the difference vector sum. If nothing (default), uses the adaptive formula 2.38 / sqrt(2 * δ * d) where d is the number of updated dimensions. If a Real is provided, uses that fixed value throughout sampling.\ncr: Crossover probability for parameter selection. Can be a Real (fixed probability), nothing (adaptive using n_cr values), or a UnivariateDistribution. If cr is a DiscreteNonParametric then it those values can also be adapted. Defaults to nothing.\nn_cr: Number of crossover probabilities to adapt between when cr is nothing. Higher values allow more fine-tuned adaptation. Defaults to 3.\nδ: Number of difference vectors to sum. Can be an Integer (fixed) or a DiscreteUnivariateDistribution (random). Defaults to DiscreteUniform(1, 3).\nϵ: Distribution for small additive noise in the selected subspace. Defaults to Uniform(-1e-4, 1e-4).\ne: Distribution for multiplicative noise (1 + e) applied to the difference vector sum. Defaults to Normal(0.0, 1e-2).\ncheck_args: Whether to validate input distributions. Defaults to true.\n\nReturns\n\nA subspace sampler that can be used with setup_sampler_scheme or step or sample from AbstractMCMC.\n\nExample\n\nusing DifferentialEvolutionMetropolis, Distributions\n\n# Setup subspace sampling with custom crossover rate and delta\nsubspace_config = setup_subspace_sampling(cr = Beta(1, 2), δ = 2)\n\nSee also setup_de_update, setup_snooker_update, setup_sampler_scheme.\n\n\n\n\n\n","category":"function"},{"location":"#AbstractMCMC.step","page":"DifferentialEvolutionMetropolis Documentation","title":"AbstractMCMC.step","text":"step(rng, model_wrapper, sampler, state; parallel=false, update_memory=true, kwargs...)\n\nPerform a single MCMC step using differential evolution sampling.\n\nThis is the core sampling function that proposes new states for all chains and accepts or rejects them according to the Metropolis criterion. For adaptive samplers, the function automatically fixes adaptive parameters before sampling.\n\nArguments\n\nrng: Random number generator\nmodel_wrapper: LogDensityModel containing the target log-density function\nsampler: Differential evolution sampler (any AbstractDifferentialEvolutionSampler)\nstate: Current state of all chains\n\nKeyword Arguments\n\nparallel: Whether to run chains in parallel using threading. Defaults to false. Advisable for slow models.\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Over writes memory options given at initialization.\nkwargs...: Additional keyword arguments passed to update functions (see https://turinglang.org/AbstractMCMC.jl/stable/api/#Common-keyword-arguments)\n\nReturns\n\nsample: DifferentialEvolutionSample containing new positions and log-densities\nnew_state: Updated state for the next iteration\n\nExample\n\nsample, new_state = step(rng, model, sampler, state; parallel=true)\n\nSee also step_warmup, sample from AbstractMCMC.\n\n\n\n\n\nstep(rng, model_wrapper, sampler; n_chains, memory=true, N₀, adapt=true, initial_position=nothing, parallel=false, kwargs...)\n\nInitialize differential evolution sampling by setting up chains and computing initial state.\n\nThis function serves as the entry point for differential evolution MCMC sampling. It handles chain initialization, memory setup for memory-based samplers, adaptive state initialization, and returns the initial sample and state that can be used with AbstractMCMC.sample.\n\nArguments\n\nrng: Random number generator\nmodel_wrapper: LogDensityModel containing the target log-density function\nsampler: Differential evolution sampler to use\n\nKeyword Arguments\n\nGeneric DE Arguments\n\nn_chains: Number of parallel chains. Defaults to max(2 * dimension, 3) for adequate mixing.\nadapt: Whether to enable adaptive behavior during warm-up (if the sampler supports it). Defaults to true.\ninitial_position: Starting positions for chains. Can be nothing (random initialization), or a vector of parameter vectors. If the provided vector is smaller than n_chains + n_hot_chains, it will be expanded; if larger and memory=true, excess positions become initial memory. Defaults to nothing.\nparallel: Whether to evaluate initial log-densities in parallel. Useful for expensive models. Defaults to false.\nn_preallocated_indices: This package provides fast sampling-without-replacement by pre-allocating indices, defaults to 3 (which the most asked for by the implemented samplers). Consider increasing it if you implement your own proposal that calls pick_chains with n_chains > 3.\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Overwrites memory options given at initialization, generally should only be of use if calling step directly.\nsilent: Suppress informational logging during initialization (e.g., initial position adjustments and   memory setup) when true. Defaults to false.\n\nMemory-based Sampling Arguments\n\nmemory: Whether to use memory-based sampling that stores past positions. Memory-based samplers can be more efficient for high-dimensional problems. Defaults to true.\nN₀: Initial memory size for memory-based samplers. Should be ≥ n_chains + n_hot_chains. Defaults to 2 * n_chains + n_hot_chains.\nmemory_size: Maximum number of positions retained per chain in memory. The effective total stored positions is   memory_size * (n_chains + n_hot_chains). Defaults to 1001 or 2*num_warmup if that is provided here or via sample. Larger sizes can improve proposal diversity but   increase memory usage. Set with consideration of available RAM and expected run length.\nmemory_refill: Whether to refill memory when full instead of extending the memory, will replace from the start. Defaults to false.\nmemory_thin_interval: Thinning interval for memory updates. If > 0, only every memory_thin_interval-th position is stored in memory.\n\nParallel Tempering and Simulated Annealing Arguments\n\nn_hot_chains: Number of hot chains for parallel tempering. Defaults to 0 (no parallel tempering).\nmax_temp_pt: Maximum temperature for parallel tempering. Defaults to 2*sqrt(dimension).\nmax_temp_sa: Maximum temperature for simulated annealing. Defaults to max_temp_pt.\nα: Temperature ladder spacing parameter. Controls the geometric spacing between temperatures. Defaults to 1.0.\nannealing: Whether to use simulated annealing (temperature decreases over time). Defaults to false.\nannealing_steps: Number of annealing steps. Defaults to annealing ? num_warmup : 0.\ntemperature_ladder: Pre-defined temperature ladder as a vector of vectors. If provided, overrides automatic temperature ladder creation. Defaults to create_temperature_ladder(n_chains, n_hot_chains, α, max_temp_pt, max_temp_sa, annealing_steps).\n\nReturns\n\nsample: DifferentialEvolutionSample containing initial positions and log-densities\nstate: Initial state (DifferentialEvolutionState) ready for sampling\n\nExamples\n\nusing DifferentialEvolutionMetropolis, Random, Distributions\n\n# Setup\nrng = Random.default_rng()\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\nsampler = deMCzs()\n\n# Basic initialization with default settings\nsample, state = step(rng, model_wrapper, sampler)\n\n# Custom number of chains with memory disabled\nsample2, state2 = step(rng, model_wrapper, sampler; n_chains=10, memory=false)\n\n# With custom initial positions\ninit_pos = [randn(2) for _ in 1:8]\nsample3, state3 = step(rng, model_wrapper, sampler; initial_position=init_pos)\n\nNotes\n\nFor non-memory samplers, n_chains should typically be ≥ dimension for good mixing\nMemory-based samplers can work effectively with fewer chains than the problem dimension\nThe function handles dimension mismatches and provides informative warnings\nInitial log-densities are computed automatically for all starting positions\nWhen using parallel tempering (n_hot_chains > 0), only the cold chains (first n_chains) are returned in the sample, but all chains participate in the sampling process\nMemory-based samplers with parallel tempering may issue warnings since hot chains typically aren't necessary when using memory\n\nSee also sample from AbstractMCMC, deMC, deMCzs, DREAMz.\n\n\n\n\n\n","category":"function"},{"location":"#AbstractMCMC.step_warmup","page":"DifferentialEvolutionMetropolis Documentation","title":"AbstractMCMC.step_warmup","text":"step_warmup(rng, model_wrapper, sampler, state; parallel=false, kwargs...)\n\nPerform a single MCMC step during the warm-up (adaptive) phase.\n\nDuring warm-up, this function performs the same sampling as step but also updates adaptive parameters. For subspace samplers, it adapts crossover probabilities based on the effectiveness of different parameter subsets.\n\nArguments\n\nrng: Random number generator\nmodel_wrapper: LogDensityModel containing the target log-density function\nsampler: Adaptive differential evolution sampler\nstate: Current state including adaptive parameters\n\nKeyword Arguments\n\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Useful if memory has grown too large.\nparallel: Whether to run chains in parallel using threading. Defaults to false.\nkwargs...: Additional keyword arguments passed to update functions\n\nReturns\n\nsample: DifferentialEvolutionSample containing new positions and log-densities\nnew_state: Updated state with adapted parameters for the next iteration\n\nExample\n\nusing DifferentialEvolutionMetropolis, Random, Distributions\n\n# Setup for warmup step example\nrng = Random.default_rng()\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\nsampler = DREAMz()\n\n# Initialize state (this would typically be done by AbstractMCMC.sample)\n# sample, new_state = step_warmup(rng, model_wrapper, sampler, state; parallel=false)\n\nSee also step, fix_sampler.\n\n\n\n\n\nstep_warmup(rng, model_wrapper, sampler, state; kwargs...)\n\nPerform a single MCMC step during warm-up for composite samplers.\n\nFor composite samplers, this function randomly selects one of the component update methods, performs a warm-up step with that method, and updates the corresponding adaptive state while preserving other component states.\n\nArguments\n\nrng: Random number generator\nmodel_wrapper: LogDensityModel containing the target log-density function\nsampler: Composite differential evolution sampler\nstate: Current state with composite adaptive parameters\n\nKeyword Arguments\n\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Useful if memory has grown too large.\nkwargs...: Additional keyword arguments passed to component update functions\n\nReturns\n\nsample: DifferentialEvolutionSample containing new positions and log-densities\nnew_state: Updated state with adapted parameters for the selected component\n\nSee also step_warmup, setup_sampler_scheme.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.fix_sampler","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.fix_sampler","text":"fix_sampler(sampler::AbstractDifferentialEvolutionSampler, adaptive_state::AbstractDifferentialEvolutionAdaptiveState)\n\nFix adaptive parameters of a sampler to their current adapted values.\n\nFor non-adaptive samplers, returns the sampler unchanged. For adaptive samplers, returns a new sampler with the adaptive parameters fixed to their current values in the adaptive_state.\n\nArguments\n\nsampler: The differential evolution sampler to fix\nadaptive_state: The adaptive state containing current parameter values\n\nReturns\n\nA sampler with fixed (non-adaptive) parameters\n\nExample\n\nusing DifferentialEvolutionMetropolis, Random, Distributions\n\n# This function is typically used after warmup/adaptation phase\n# fixed_sampler = fix_sampler(adaptive_sampler, state.adaptive_state)\n\nSee also fix_sampler_state.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.fix_sampler_state","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.fix_sampler_state","text":"fix_sampler_state(sampler::AbstractDifferentialEvolutionSampler, state::DifferentialEvolutionState)\n\nFix adaptive sampler parameters and return a corresponding non-adaptive state.\n\nTakes an adaptive sampler and state, fixes the sampler's adaptive parameters to their current values, and returns both the fixed sampler and a simplified state without adaptive components.\n\nArguments\n\nsampler: The differential evolution sampler (potentially adaptive)\nstate: The current sampler state (DifferentialEvolutionState)\n\nReturns\n\nfixed_sampler: Sampler with adaptive parameters fixed to current values\nfixed_state: State without adaptive components\n\nExample\n\nusing DifferentialEvolutionMetropolis, Random, Distributions\n\n# This function is typically used after warmup/adaptation phase\n# fixed_sampler, fixed_state = fix_sampler_state(sampler, state)\n\nSee also fix_sampler.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.r̂_stopping_criteria","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.r̂_stopping_criteria","text":"r̂_stopping_criteria(rng, model, sampler, samples, state, iteration; kwargs...)\n\nStopping criterion based on the Gelman-Rubin diagnostic (R̂).\n\nSampling continues until the R̂ value for all parameters falls below maximum_R̂, indicating convergence across chains. This function is designed to be used as the N_or_isdone argument in AbstractMCMC.sample for adaptive stopping.\n\nThe diagnostic is computed on the last half of the collected samples to focus on the stationary portion of the chains.\n\nArguments\n\nrng: Random number generator (unused but required by AbstractMCMC interface)\nmodel: The model being sampled (unused but required by interface)\nsampler: The differential evolution sampler (unused but required by interface)\nsamples: Vector of collected samples from all chains\nstate: Current sampler state (unused but required by interface)\niteration: Current iteration number\n\nKeyword Arguments\n\ncheck_every: Frequency (in iterations) for checking R̂ values. Defaults to 1000.\nmaximum_R̂: Maximum acceptable R̂ value for convergence. Defaults to 1.2.\nmaximum_iterations: Maximum number of iterations before forced stopping. Defaults to 100000.\nminimum_iterations: Minimum iterations before convergence checking begins. Defaults to 0.\n\nReturns\n\ntrue if sampling should stop (converged or maximum iterations reached)\nfalse if sampling should continue\n\nExample\n\nusing DifferentialEvolutionMetropolis, AbstractMCMC, Random, Distributions\n\n# Create a simple model\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\n\n# Setup sampler\nsampler = deMCzs()\n\n# Use with adaptive stopping criterion\nrng = Random.default_rng()\nchains = sample(rng, model_wrapper, sampler, r̂_stopping_criteria;\n               n_chains=4, check_every=500, maximum_R̂=1.1)\n\nSee also MCMCDiagnosticTools.rhat, deMCzs, DREAMz.\n\n\n\n\n\n","category":"function"},{"location":"#DifferentialEvolutionMetropolis.DifferentialEvolutionOutput","page":"DifferentialEvolutionMetropolis Documentation","title":"DifferentialEvolutionMetropolis.DifferentialEvolutionOutput","text":"DifferentialEvolutionOutput{T <: Real}\n\nContainer for differential evolution MCMC sampling results.\n\nFields\n\nsamples::Array{T, 3}: Three-dimensional array of parameter samples with dimensions (iterations, chains, parameters). Each sample represents a point in parameter space from the MCMC chain.\nld::Matrix{T}: Two-dimensional matrix of log-density values with dimensions (iterations, chains). Contains the log-probability density evaluated at each corresponding sample point.\n\nType Parameters\n\nT <: Real: Numeric type for the samples and log-density values (typically Float64).\n\nExamples\n\n# Access samples from the output\noutput = sample(model, sampler, n_samples)\nparameter_samples = output.samples  # Shape: (n_samples, n_chains, n_params)\nlog_densities = output.ld          # Shape: (n_samples, n_chains)\n\n# Extract samples for a specific chain\nchain_1_samples = output.samples[:, 1, :]  # All samples from chain 1\n\n\n\n\n\n","category":"type"},{"location":"tutorial/#Sampling-from-multimodal-distributions","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"Let say you have a multimodal distribution, for example a mixture of two Gaussians. DifferentialEvolutionMetropolis implements differential evolution MCMC samplers (including deMC-zs and DREAMz) that are designed to sample from such distributions efficiently. Roughly these samplers work by generating new proposals based on many separate chains (or a history of sampled chains). In theory this allows the sampler to easily jump between modes of the distribution.","category":"section"},{"location":"tutorial/#Multimodal-Distributions","page":"Sampling from multimodal distributions","title":"Multimodal Distributions","text":"First we need to implement a multimodal problem. We will assume that our data is generated by a mixture of two 2D-Gaussians with means 5 and -5 and a 65% bias towards the positive value. Then the distributions of the means will be multimodal and highly correlated with the probability. We can easily implement this using the Distributions package, which underpins most of the functionality in DifferentialEvolutionMetropolis.\n\nusing Distributions, Random\n\nmean = 5.0\nstd = 2.0\np = 0.65\n\ndist = MixtureModel([\n    Normal(mean, std),\n    Normal(-mean,  std)\n], [p, 1 - p]);\n\n#generate our data\nRandom.seed!(1234)\ndata = rand(dist, 1000)\n\n# setup our log density\nstruct MixtureNormal\n    #data\n    data::Vector{Float64}\n    #prior parameters\n    prior_μ_σ::Float64\n    prior_σ_σ::Float64\nend\n\n#initialize the model\nmodel = MixtureNormal(data, 10.0, 10.0)\n\n# implement the log density function\nfunction (problem::MixtureNormal)(θ)\n    (; μ, p, σ) = θ\n    ld = \n        #priors\n        logpdf(Normal(0, problem.prior_σ_σ), σ) + #half-normal because σ > 0\n        logpdf(Uniform(0, 1), p) #prior on mixture weight\n    μ_prior = Normal(0, problem.prior_μ_σ)\n    for μ_ in μ\n        ld += logpdf(μ_prior, μ_)\n    end\n    #likelihood\n    ll_model = MixtureModel(\n        [\n            Normal(μ[1], σ),\n            Normal(μ[2], σ)\n        ], [p, 1 - p]\n    )\n    for i in axes(problem.data, 1)\n        ld += logpdf(ll_model, data[i])\n    end\n    return ld\nend\n\n# to visualize our data\nusing Plots\nplot(histogram(data, bins = 30, xlabel = \"Value\", ylabel = \"Frequency\", title = \"Histogram of Generated Data\"))\n\nWe can also transform our log density function, so we can provide real-valued inputs. This is much easier to work with.\n\nusing TransformedLogDensities, TransformVariables\n\ntransformation = as((μ = as(Array, 2), p = as𝕀, σ = asℝ₊))\ntransformed_ld = TransformedLogDensity(transformation, model)","category":"section"},{"location":"tutorial/#Sampling-with-DifferentialEvolutionMetropolis","page":"Sampling from multimodal distributions","title":"Sampling with DifferentialEvolutionMetropolis","text":"Now let's use DifferentialEvolutionMetropolis to sample from this multimodal distribution. Here we use the DREAMz sampler, which is well-suited for exploring complex, multimodal spaces. We increase the number of chains to allow the sampler to explore the distribution more effectively.\n\nusing DifferentialEvolutionMetropolis, AbstractMCMC\n\nmodel = AbstractMCMC.LogDensityModel(transformed_ld)\n\n# Sample using DREAMz with adaptive stopping based on convergence\ndreamz = DREAMz(model, 10000; n_chains = 6, progress = true);\n\nOther implementations of the differential evolution MCMC algorithm are available in DifferentialEvolutionMetropolis.jl, such as deMC and deMCzs, which can be used similarly.","category":"section"},{"location":"tutorial/#Custom-Scheme","page":"Sampling from multimodal distributions","title":"Custom Scheme","text":"DREAMz can be further customized. For example, we could include snooker updates alongside the DREAMz-like subspace sampling.\n\nYou can also modify aspects of the implemented sampling, for example tell DREAMz to use non-memory-based sampling with DREAMz(...; memory = false), or you can define your own sampler scheme for more control over the sampling process.\n\nThis time we will use MCMCChains.jl to the handle the output.\n\nusing MCMCChains\n\n# Create a custom sampler scheme combining different update types\ncustom_sampler = setup_sampler_scheme(\n    setup_subspace_sampling(), # a DREAM-like sampler that uses subspace sampling\n    setup_snooker_update(deterministic_γ = false), # a snooker update for better exploration\n    setup_de_update(); # standard DE update\n    w = [0.6, 0.2, 0.2] # weights for each update type\n);\n\n# Sample using AbstractMCMC.sample with custom stopping criteria\ncustom_results = sample(\n    model,\n    custom_sampler,\n    r̂_stopping_criteria;\n    check_every = 10000,\n    maximum_R̂ = 1.05,\n    n_chains = 4,\n    N₀ = 100, # want good exploration of the space\n    memory = true,\n    parallel = true,\n    annealing = true,\n    num_warmup = 10000,\n    memory_size = 5000,\n    memory_refill = true,\n    chain_type = MCMCChains.Chains\n);\n\nYou can also define your own samplers for more specialized use cases by extending the abstract types.","category":"section"},{"location":"tutorial/#Interpreting-Results","page":"Sampling from multimodal distributions","title":"Interpreting Results","text":"After running the sampler, you will have a collection of samples from the target distribution. These samples can be used to estimate summary statistics, credible intervals, and to assess the quality of your sampling.","category":"section"},{"location":"tutorial/#Assessing-Sampler-Performance:-ESS-and-R-hat","page":"Sampling from multimodal distributions","title":"Assessing Sampler Performance: ESS and R-hat","text":"To evaluate how well your sampler is performing, you can compute the effective sample size (ESS) and the R-hat diagnostic. These metrics help you determine if your chains have mixed well and if your estimates are reliable.\n\nEffective Sample Size (ESS): This measures the number of independent samples your chains are equivalent to. Higher ESS values indicate more reliable estimates.\nR-hat Diagnostic: Also known as the Gelman-Rubin statistic, R-hat compares the variance within each chain to the variance between chains. Values close to 1 suggest good mixing and convergence; values much greater than 1 indicate potential problems.\n\nBelow is an example of how to compute these diagnostics for the DifferentialEvolutionMetropolis samplers using MCMCDiagnosticTools:\n\nusing Statistics, MCMCDiagnosticTools\n\n# Compute diagnostics for DREAMz results\ness_val = ess(dreamz.samples) ./ size(dreamz.samples, 1)\nrhat_val = maximum(rhat(dreamz.samples))\n\nprintln(\"DREAMz diagnostics:\")\nprintln(\"  ESS per iteration: $ess_val\")\nprintln(\"  R-hat: $rhat_val\")\n\n# example trace plot\nplot(dreamz.samples[:, :, 1], xlabel = \"Iteration\", ylabel = \"Value\", title = \"Trace Plot for Mean 1 (DREAMz)\")","category":"section"},{"location":"tutorial/#Summarizing-Posterior-Samples","page":"Sampling from multimodal distributions","title":"Summarizing Posterior Samples","text":"Once you have confirmed good mixing and convergence, you can summarize your posterior samples. For each parameter, you may want to compute the median and a credible interval (such as the 90% interval):\n\n#need to transform the samples to original space, there is currently no nice way to do this\ncustom_results = cat([dreamz.samples[:, c, :] for c in axes(dreamz.samples, 2)]...; dims = 1)\nfor it in axes(custom_results, 1)\n    transformed_values = transform(transformation, Array(custom_results[it, :]))\n    custom_results[it, :] .= [transformed_values.μ..., transformed_values.p, transformed_values.σ]\nend\n\n#median for variance\nmed = median(custom_results[:, 4])\nq05, q95 = quantile(custom_results[:, 4], [0.05, 0.95])\nprintln(\"Standard deviation: posterior median = $med, 90% CI = ($q05, $q95)\")\nprintln(\"True standard deviation: $std\")\n\n#not so sensible for p or means\nmed = median(custom_results[:, 1])\nq05, q95 = quantile(custom_results[:, 1], [0.05, 0.95])\nprintln(\"Mean 1: posterior median = $med, 90% CI = ($q05, $q95)\")\nprintln(\"True mean: $(mean)\")\n\n#visualize the posteriors, expect multimodal distributions\nplot(histogram(custom_results[:, 1], bins = 30, xlabel = \"Mean 1\", ylabel = \"Frequency\"), histogram(custom_results[:, 2], bins = 30, xlabel = \"Mean 2\", ylabel = \"Frequency\"), histogram(custom_results[:, 3], bins = 30, xlabel = \"Probability\", ylabel = \"Frequency\"), layout = (3, 1))\n\n#covariance\nplot(scatter(custom_results[:, 1], custom_results[:, 3], xlabel = \"Mean 1\", ylabel = \"Probability of Normal 1\"), scatter(custom_results[:, 1], custom_results[:, 2], xlabel = \"Mean 1\", ylabel = \"Mean 2\"), layout = (2, 1))\n\nFor more details, see the DifferentialEvolutionMetropolis Documentation and Customizing your sampler.","category":"section"}]
}
